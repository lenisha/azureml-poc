{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Deploy multiple machine learning models from registry to online endpoint  \n",
        "\n",
        "Learn how to use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure. You'll begin by deploying a model on your local machine to debug any errors, and then you'll deploy and test it in Azure.\n",
        "\n",
        "Managed online endpoints help to deploy your ML models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure. \n",
        "\n",
        "For more information, see [What are Azure Machine Learning endpoints?](https://docs.microsoft.com/azure/machine-learning/concept-endpoints)."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
        "\n",
        "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
        "\n",
        "* You must have an Azure Machine Learning workspace. \n",
        "\n",
        "* Train Iris model and register multiple versions [Iris Training](./train-iris-scikit-learn.ipynb)\n",
        "\n",
        "## Steps\n",
        "\n",
        "* Connect to Workspace\n",
        "\n",
        "* Create Endpoint\n",
        "\n",
        "* Add Role 'AzureML DataScientist'  to Endpoint Managed Identity one Workspace scope \n",
        "\n",
        "* Create Deployment with scoring scipt\n",
        "\n",
        "* Scoring Scipt Loads  multiple models from WS  Registry "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "\n",
        "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
        "\n",
        "## 1.1. Import the required libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1683687582751
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Configure workspace details and get a handle to the workspace\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Enter details of your AML workspace\n",
        "subscription_id = \"SUB\"\n",
        "resource_group = \"RG\"\n",
        "workspace_name = \"WS\"\n",
        "\n",
        "\n",
        "user_identity_id=\"/subscriptions/...../providers/Microsoft.ManagedIdentity/userAssignedIdentities/...\"\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1683687582938
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1683687583261
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Deploy your online endpoint to Azure\n",
        "Next, deploy your online endpoint to Azure.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 4.1 Create  Managed Endpoint Identity and add Role\n",
        "To access ML Models from Registry in workspace Managed Online Endpoint System Identity needs to be granted access to registry.\n",
        "Assign `AzureML Data Scientist` role to MOE Identity on Workspace scope\n",
        "![identity](docs/modelidentity.png)\n",
        "\n",
        "- Reader on Workspace\n",
        "- ACRPull on ACR\n",
        "- Blob Storage Reader on Storage\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?view=azureml-api-2&tabs=cli#authorization-error "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Configure online endpoint\r\n",
        "`endpoint_name`: The name of the endpoint. It must be unique in the Azure region. Naming rules are defined under [managed online endpoint limits](https://docs.microsoft.com/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints-preview).\r\n",
        "\r\n",
        "`auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` does not expire, but `aml_token` does expire. \r\n",
        "\r\n",
        "Optionally, you can add description, tags to your endpoint."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
        "import datetime\n",
        "from azure.ai.ml.entities import ManagedIdentityConfiguration, IdentityConfiguration\n",
        "\n",
        "online_endpoint_name = \"multimodel-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is a multimodel online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    identity=IdentityConfiguration(\n",
        "        type=\"user_assigned\",\n",
        "        user_assigned_identities=[\n",
        "            ManagedIdentityConfiguration(resource_id=user_identity_id)\n",
        "        ],\n",
        "    ),\n",
        "    tags={\"foo\": \"bar\"},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1683687583648
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.1 Create the endpoint\n",
        "\n",
        "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\nReadonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://multimodel-05100259870723.eastus2.inference.ml.azure.com/score', 'openapi_uri': 'https://multimodel-05100259870723.eastus2.inference.ml.azure.com/swagger.json', 'name': 'multimodel-05100259870723', 'description': 'this is a multimodel online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f9b97038-ed78-4a26-a1a7-51e81e75d867/resourcegroups/openaml/providers/microsoft.machinelearningservices/workspaces/nlp-workspace/onlineendpoints/multimodel-05100259870723', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f9b97038-ed78-4a26-a1a7-51e81e75d867/providers/Microsoft.MachineLearningServices/locations/eastus2/mfeOperationsStatus/oe:baa4dabf-18ba-45e2-8649-6d72d7082169:83f1d8e6-1b46-4e57-9da8-94c0d3d0d39a?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f9b97038-ed78-4a26-a1a7-51e81e75d867/resourceGroups/openaml/providers/Microsoft.MachineLearningServices/workspaces/nlp-workspace/onlineEndpoints/multimodel-05100259870723', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/eneros2/code/Users/eneros/azureml-poc/multimodel-registry', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa369ffb580>, 'auth_mode': 'key', 'location': 'eastus2', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fa369ffb700>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1683687677649
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(online_endpoint_name)\n",
        "print(\"Endpoint Identity {0} \".format(endpoint.identity))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpoint Identity <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fa369ff8f10> \n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1683687677979
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 4.3 Configure online deployment\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 4.3.1 Explore Scoring script\n",
        "\n",
        "Scoring script is in `onlinescoring/score_registry.py` reads MLFlow Tracking URI from environment variable  (set during registration) and loads models from workspace for prediction\n",
        "\n",
        "```python\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model1, model2\n",
        "    # More details  https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models-mlflow\n",
        "    mlflow.set_tracking_uri(os.getenv(\"TRACKING_URI\"))\n",
        "    logging.info(\"Tracking URL: {0}\".format(os.getenv(\"TRACKING_URI\")))\n",
        "\n",
        "    model1 = mlflow.sklearn.load_model(\"models:/iris_svc_model/1\")\n",
        "    model2 = mlflow.sklearn.load_model(\"models:/iris_svc_model/latest\")\n",
        "    logging.info(\"Init complete\")\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    In the example we extract the data from the json input and call the scikit-learn model's predict()\n",
        "    method and return the result back\n",
        "    \"\"\"\n",
        "    logging.info(\"Request received\")\n",
        "    json_data = json.loads(raw_data)\n",
        "    if \"input_data\" not in json_data.keys():\n",
        "        raise Exception(\"Request must contain a top level key named 'input_data'\")\n",
        "  \n",
        "    \n",
        "    input_data = json_data[\"input_data\"]\n",
        "    print(json.dumps(input_data))\n",
        "\n",
        "    data = input_data[\"data\"]\n",
        "    data = numpy.array(data)\n",
        "    num_rows, num_cols = data.shape # just example based on number of rows to switch model\n",
        "\n",
        "    if ( num_rows <= 2 ) :\n",
        "      logging.info(\" Model 1 is predicting\")\n",
        "      result = model1.predict(data)\n",
        "    else:\n",
        "      logging.info(\" Model 2 is predicting\")\n",
        "      result = model2.predict(data)  \n",
        "    logging.info(\"Request processed\")\n",
        "    return result.tolist()\n",
        "\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## 4.3.2 Register Deployment\n",
        "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(\n",
        "    conda_file=\"./environment/conda.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")\n",
        "workspace = ml_client.workspaces.get(name=workspace_name)\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\"),\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    #model=model,\n",
        "    environment=env,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"./onlinescoring\", scoring_script=\"score_registry_uai.py\"\n",
        "    ),\n",
        "    environment_variables={\n",
        "        \"TRACKING_URI\": workspace.mlflow_tracking_uri,\n",
        "        \"UAI_CLIENT_ID\": user_identity_id\n",
        "    },\n",
        "    instance_type=\"Standard_F4s_v2\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "print(\"Deployment {0}  defined\".format(blue_deployment.name))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Deployment blue05100417692517  defined\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1683692226719
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Create the deployment\n",
        "\n",
        "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint multimodel-05100259870723 exists\n\u001b[32mUploading onlinescoring (0.0 MBs): 100%|██████████| 2891/2891 [00:00<00:00, 49612.87it/s]\n\u001b[39m\n\ndata_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "................................................................"
        },
        {
          "output_type": "error",
          "ename": "HttpResponseError",
          "evalue": "(None) ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nCode: None\nMessage: ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nException Details:\t(None) ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"nlpworkspace8557870865\" under the path \"/azureml/ImageLogs/11ded7f8-0d35-422e-bf2a-33f3547358da/build.log\"\n\tCode: None\n\tMessage: ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"nlpworkspace8557870865\" under the path \"/azureml/ImageLogs/11ded7f8-0d35-422e-bf2a-33f3547358da/build.log\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationFailed\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:514\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadStatus \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:554\u001b[0m, in \u001b[0;36mLROBasePolling._poll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _failed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus()):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationFailed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation failed or canceled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    556\u001b[0m final_get_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mget_final_get_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response)\n",
            "\u001b[0;31mOperationFailed\u001b[0m: Operation failed or canceled",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblue_deployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:247\u001b[0m, in \u001b[0;36mLROPoller.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# type: (Optional[int]) -> PollingReturnType\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the result of the long running operation, or\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    the result available after the specified timeout.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    :raises ~azure.core.exceptions.HttpResponseError: Server problem with the query.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_polling_method\u001b[38;5;241m.\u001b[39mresource()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:267\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m: \u001b[38;5;66;03m# Was None\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/_poller.py:184\u001b[0m, in \u001b[0;36mLROPoller._start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"Start the long running operation.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03mOn completion, runs any callbacks.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m:param callable update_cmd: The API request to check the status of\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m the operation.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_polling_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/polling/base_polling.py:532\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    526\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response,\n\u001b[1;32m    527\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(err),\n\u001b[1;32m    528\u001b[0m         error\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailed \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(\n\u001b[1;32m    533\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response,\n\u001b[1;32m    534\u001b[0m         error\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m    535\u001b[0m     )\n",
            "\u001b[0;31mHttpResponseError\u001b[0m: (None) ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nCode: None\nMessage: ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\nException Details:\t(None) ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"nlpworkspace8557870865\" under the path \"/azureml/ImageLogs/11ded7f8-0d35-422e-bf2a-33f3547358da/build.log\"\n\tCode: None\n\tMessage: ResourceNotReady: User container has crashed or terminated. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-resourcenotready\n\tThe build log is available in the workspace blob store \"nlpworkspace8557870865\" under the path \"/azureml/ImageLogs/11ded7f8-0d35-422e-bf2a-33f3547358da/build.log\""
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1683692576039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# blue deployment takes 100 traffic\n",
        "endpoint.traffic = {f\"{blue_deployment.name}\": 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683688408863
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Test the endpoint with sample data\n",
        "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
        "- `endpoint_name` - Name of the endpoint\n",
        "- `request_file` - File with request data\n",
        "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
        "\n",
        "We will send a sample request using a [json](./model-1/sample-request.json) file. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test the blue deployment with some sample data\n",
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=blue_deployment.name,\n",
        "    request_file=\"./onlinescoring/sample-request.json\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683688408885
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Managing endpoints and deployments\n",
        "\n",
        "## 6.1 Get details of the endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the details for online endpoint\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683688408902
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Get the logs for the new deployment\n",
        "Get the logs for the green deployment and verify as needed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=blue_deployment.name, endpoint_name=online_endpoint_name, lines=50\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683688408921
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Delete the endpoint\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683688408937
        }
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "78f9db5fbd37e8889e0f83cef79d3f22c09395ee4e0648cc45e2c02045ffa952"
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "description": {
      "description": "Use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}